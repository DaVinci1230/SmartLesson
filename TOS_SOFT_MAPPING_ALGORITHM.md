# TOS Soft-Mapping Assignment Algorithm (SSMA)

## Overview

The **Soft-Mapping Assignment Algorithm (SSMA)** is the bridge between assessment design and test implementation. It takes:

1. âœ… **TOS Bloom Distribution** (locked) - WHAT to assess
2. âœ… **Question Type Distribution** (locked) - HOW to assess

And produces:

ðŸŽ¯ **Complete Exam Blueprint** - Each question's cognitive level, format, and point value

---

## Problem It Solves

### Before SSMA
- TOS defines Bloom items per outcome: "Outcome 1: 5 Remember, 3 Apply"
- Teachers define question types: "25 MCQ @ 1 point, 3 Essay @ 5 points"
- How to map them? No systematic approach.
- Assignments were manual and error-prone.

### After SSMA
- Automated, deterministic mapping
- Preserves exact distributions
- Intelligent soft-preference matching
- No manual work required
- 100% integrity guaranteed

---

## How It Works

### STEP 1: Expand Bloom Slots

Takes TOS matrix and expands each distribution cell into individual slots:

**Input:**
```python
tos_matrix = {
    "Remember": {0: 5, 1: 3},      # Outcome 0: 5 items, Outcome 1: 3 items
    "Apply": {0: 4, 1: 6}
}
outcomes = [
    {"id": 0, "text": "Define concepts"},
    {"id": 1, "text": "Classify items"}
]
```

**Output:**
```python
[
    BloomSlot(outcome_id=0, outcome_text="Define concepts", bloom_level="Remember"),
    BloomSlot(outcome_id=0, outcome_text="Define concepts", bloom_level="Remember"),
    ... (5 Remember slots for outcome 0)
    BloomSlot(outcome_id=1, outcome_text="Classify items", bloom_level="Remember"),
    ...
]
# Total: 18 individual slots
```

### STEP 2: Expand Type Slots

Takes question type configuration and expands into individual slots:

**Input:**
```python
question_types = [
    QuestionType("MCQ", 10, 1),        # 10 MCQ @ 1 point each
    QuestionType("Essay", 3, 5),       # 3 Essay @ 5 points each
    QuestionType("Problem", 5, 3)      # 5 Problem Solving @ 3 points each
]
```

**Output:**
```python
[
    TypeSlot(question_type="MCQ", points_per_item=1),
    TypeSlot(question_type="MCQ", points_per_item=1),
    ... (10 MCQ, each remembering 1 point)
    TypeSlot(question_type="Essay", points_per_item=5),
    TypeSlot(question_type="Essay", points_per_item=5),
    TypeSlot(question_type="Essay", points_per_item=5),
    ...
]
# Total: 18 type slots (must equal Bloom slots!)
```

**CRITICAL: Points are PRESERVED, not redistributed**
- Teachers configured "MCQ: 1 point each" â€” we keep that exactly
- Not "18 items Ã· 18 points = 1 point each"
- Teacher intent is locked in

---

### STEP 3: Soft-Preference Mapping

For each Bloom slot, intelligently assign a type slot using preferences:

**Preference Dictionary:**
```python
PREFERRED_TYPES = {
    "Remember": ["MCQ", "Identification"],           # Quick recall
    "Understand": ["MCQ", "Short Answer"],           # Concept check
    "Apply": ["MCQ", "Problem Solving"],             # Context use
    "Analyze": ["Short Answer", "Problem Solving"],  # Decomposition
    "Evaluate": ["Essay", "Problem Solving"],        # Judgment
    "Create": ["Essay", "Drawing/Diagram"]           # Original work
}
```

**Algorithm:**
```
For each Bloom slot:
    1. Look at its Bloom level (e.g., "Analyze")
    2. Get preferred types for that level: ["Short Answer", "Problem Solving"]
    3. Check if available in type pool:
       a. If "Short Answer" exists â†’ use it (preferred match)
       b. Else if "Problem Solving" exists â†’ use it (preferred match)
       c. Else â†’ use ANY remaining type (fallback match)
    4. Remove assigned type from pool
    5. Next Bloom slot
```

**Example:**
```
Bloom Slots:
  [Remember, Remember, Apply, Analyze, Create] = 5 slots

Type Slots (pool):
  [MCQ(1pt), Essay(5pt), Essay(5pt), Problem(3pt)] = 4 slots

ERROR! 5 â‰  4. Integrity violated. Raise exception.

Type Slots (corrected):
  [MCQ(1pt), ShortAnswer(2pt), Essay(5pt), Problem(3pt), Essay(5pt)] = 5

Mapping:
  Remember â†’ MCQ (preferred) âœ“
  Remember â†’ ShortAnswer (next best available)
  Apply â†’ MCQ? (used) â†’ ShortAnswer? (used) â†’ Problem (fallback)
  Analyze â†’ ShortAnswer? (used) â†’ Problem (preferred match!)
  Create â†’ Essay (preferred) âœ“

Result:
  Remember â€” MCQ (1 pt)
  Remember â€” ShortAnswer (2 pts)
  Apply â€” Problem (3 pts)
  Analyze â€” Problem? (used) â€” Essay (5 pts) (fallback)
  Create â€” Essay (5 pts)
```

### STEP 4: Final Structure

**Output:**
```python
[
    AssignedSlot(
        outcome_id=0,
        outcome_text="Define concepts",
        bloom_level="Remember",
        question_type="MCQ",
        points=1
    ),
    AssignedSlot(
        outcome_id=0,
        outcome_text="Define concepts",
        bloom_level="Remember",
        question_type="Short Answer",
        points=2
    ),
    ...
]
```

Optionally shuffled for realistic exam distribution â†’ ready for test generation!

---

## Key Design Decisions

### 1. Why Soft Preferences (Not Hard)?

**Hard Rule**: "Always use MCQ for Remember"  
**Problem**: What if teacher configured 0 MCQs? No Remember items? Fails.

**Soft Rule**: "Prefer MCQ for Remember, but use anything if unavailable"  
**Benefit**: Always works, even with unusual type distributions

### 2. Why NOT Auto-Assign Points?

**Wrong Approach**: "18 items, 90 points â†’ 5 points each"  
**Why wrong**: Ignores teacher's intent
```
Teacher configured:
  - MCQ: 40 items @ 1 point (quick check)
  - Essay: 2 items @ 10 points (deep thinking)

Auto-assign would make ALL items ~2 points, destroying intent!
```

**Correct Approach**: Preserve exact point configuration  
```
We create 40 MCQ slots @ 1 point,
           2 Essay slots @ 10 points

Then map them to Bloom levels.

Total points = 40Ã—1 + 2Ã—10 = 60 (exact!)
```

### 3. Why Shuffle at the End?

Exams shouldn't have all Remember items first, then all Create items.
Shuffle provides realistic question order while preserving all constraints:
- Bloom distribution stayed exact âœ“
- Type distribution stayed exact âœ“
- Point values stayed exact âœ“
- Just order changed (not data) âœ“

---

## Data Integrity Guarantees

The algorithm GUARANTEES:

âœ… **Exact Bloom Distribution**
```python
# TOS says Remember: 10 items
# Result will have exactly 10 items with bloom="Remember"
```

âœ… **Exact Type Distribution**
```python
# Config says MCQ: 25 items
# Result will have exactly 25 items with type="MCQ"
```

âœ… **Exact Points**
```python
# Config says MCQ: 1 point each
# Every MCQ slot will have points=1 (not auto-calculated)
# Total: 25 Ã— 1 = 25 points (preserved)
```

âœ… **No Duplication**
```python
# Every type slot used exactly once
# No type slot reused
# No gaps
```

âœ… **100% Coverage**
```python
# Every Bloom slot filled
# Every type slot assigned
# Nothing left over
```

---

## API Reference

### Main Function

```python
def assign_question_types_to_bloom_slots(
    tos_matrix: Dict[str, Dict[int, int]],
    outcomes: List[Dict],
    question_types_list: List,
    shuffle: bool = True
) -> Tuple[List[AssignedSlot], Dict[str, any]]:
    """
    Assign question types to Bloom slots using soft-preference mapping.
    
    Args:
        tos_matrix: TOS distribution {bloom_level: {outcome_id: count}}
        outcomes: List of outcomes with id and text
        question_types_list: List of QuestionType objects
        shuffle: Randomize final order (default: True)
    
    Returns:
        Tuple of:
        - assigned_slots: List of AssignedSlot objects (exam blueprint)
        - metadata: Mapping statistics and validation info
    
    Raises:
        ValueError: If Bloom slots â‰  Type slots (integrity violation)
    """
```

### Data Classes

#### BloomSlot
```python
@dataclass
class BloomSlot:
    outcome_id: int           # Which outcome
    outcome_text: str         # Outcome description
    bloom_level: str          # Remember/Understand/Apply/etc.
    
    def to_dict() -> Dict:    # Serialize
```

#### TypeSlot
```python
@dataclass
class TypeSlot:
    question_type: str        # MCQ, Essay, etc.
    points_per_item: float    # Exact points (not auto-calculated)
    
    def to_dict() -> Dict:    # Serialize
```

#### AssignedSlot (Output)
```python
@dataclass
class AssignedSlot:
    outcome_id: int           # Which outcome
    outcome_text: str         # Outcome description
    bloom_level: str          # Remember/Understand/Apply/etc.
    question_type: str        # MCQ, Essay, etc.
    points: float             # Exact points (from type config)
    
    def to_dict() -> Dict:    # Serialize
```

### Utility Functions

```python
def verify_assignment_integrity(
    assigned_slots: List[AssignedSlot],
    tos_matrix: Dict[str, Dict[int, int]],
    question_types_list: List
) -> Tuple[bool, List[str]]:
    """Verify assignment preserves all distributions."""

def get_assignment_summary(
    assigned_slots: List[AssignedSlot]
) -> Dict:
    """Generate statistics by Bloom level and question type."""

def format_assignment_for_export(
    assigned_slots: List[AssignedSlot]
) -> List[Dict]:
    """Format for JSON/database storage."""
```

---

## Example Usage

```python
from services.tos_slot_assignment_service import assign_question_types_to_bloom_slots
from services.question_type_service import QuestionType

# TOS data (from assess_tos tab)
tos_matrix = {
    "Remember": {0: 10, 1: 5},
    "Understand": {0: 8, 1: 7},
    "Apply": {0: 6, 1: 4}
}
outcomes = [
    {"id": 0, "text": "Define concepts"},
    {"id": 1, "text": "Apply procedures"}
]

# Question type config (from teacher)
question_types = [
    QuestionType("MCQ", 15, 1),
    QuestionType("Short Answer", 10, 2),
    QuestionType("Problem Solving", 5, 3)
]

# Run assignment
assigned, metadata = assign_question_types_to_bloom_slots(
    tos_matrix,
    outcomes,
    question_types,
    shuffle=True
)

# Results
print(f"Assigned {len(assigned)} questions")
print(f"Coverage quality: {metadata['coverage_quality']}")

# Each item in assigned is ready for test question generation:
for item in assigned:
    print(f"  {item.outcome_text} â†’ {item.bloom_level}")
    print(f"    Type: {item.question_type} ({item.points} pts)")
```

---

## Test Coverage

8 comprehensive tests verify:

âœ… **TEST 1**: Bloom slot expansion (matrix â†’ individual slots)  
âœ… **TEST 2**: Type slot expansion (preserves points exactly)  
âœ… **TEST 3**: Basic soft-preference mapping (10 items)  
âœ… **TEST 4**: Fallback behavior (preferred types unavailable)  
âœ… **TEST 5**: Complex multi-outcome (68 items, 4 outcomes, 4 Bloom levels)  
âœ… **TEST 6**: Integrity verification (detects corruption)  
âœ… **TEST 7**: Summary generation (statistics by bloom/type)  
âœ… **TEST 8**: Shuffle behavior (randomization works)  

**Result**: 100% pass rate âœ…

---

## Integration Points

### For Streamlit UI (not yet implemented)
```python
# After teacher finalizes TOS and question types:
assigned, _ = assign_question_types_to_bloom_slots(
    st.session_state.generated_tos["tos_matrix"],
    st.session_state.generated_tos["outcomes"],
    st.session_state.question_types,
    shuffle=True
)

# Store in session state
st.session_state.exam_blueprint = assigned

# Display in Generate Test Questions tab
st.dataframe(pd.DataFrame([s.to_dict() for s in assigned]))
```

### For AI Test Question Generation (future)
```python
# TQS module will use exam blueprint:
for exam_slot in exam_blueprint:
    # Generate question matching:
    # - Learning outcome
    # - Bloom level (cognitive difficulty)
    # - Question type format
    # - Point value
    
    question = generate_question(
        outcome_text=exam_slot.outcome_text,
        bloom_level=exam_slot.bloom_level,
        question_type=exam_slot.question_type,
        points=exam_slot.points
    )
```

---

## Files

| File | Purpose | Lines |
|------|---------|-------|
| `services/tos_slot_assignment_service.py` | Service module with algorithm | 500+ |
| `test_tos_slot_assignment.py` | Verification tests | 400+ |
| `TOS_SOFT_MAPPING_ALGORITHM.md` | This documentation | 450+ |

---

## Why This Design Matters

### For Teachers
- No manual question assignment
- Automatic alignment of question type with cognitive level
- Confidence that exam structure matches educational design
- Flexibility to use unusual question type distributions

### For Students
- Exams match course content (Bloom alignment)
- Question types appropriate for assessment level
- Fair point distribution reflecting question complexity
- Realistic exam ordering (not predictable patterns)

### For Administrators
- Deterministic, auditable process
- Export-ready exam blueprints
- Facilitates curriculum alignment verification
- Supports measurement of learning outcomes

### For Developers
- Clear separation of concerns
- Single responsibility per function
- Comprehensive test coverage
- Well-documented constraints and assumptions
- Ready for AI test generation (TQS)

---

## Status

âœ… **Complete and Tested**
- Service module: 500+ lines of production code
- Test suite: 8 categories, 100% pass rate
- Documentation: Complete and detailed
- Ready for: Streamlit integration and TQS development

**Next Steps**:
1. Integrate into Streamlit UI (Generate Test Questions tab)
2. Create TQS module using exam blueprint
3. Add AI-assisted question generation

---

## References

- [Question Type Service](../services/question_type_service.py) - Point configuration
- [TOS Service](../services/tos_service.py) - Bloom distribution
- [Test Suite](../test_tos_slot_assignment.py) - Verification
- [Bloom's Taxonomy](https://en.wikipedia.org/wiki/Bloom%27s_taxonomy) - Cognitive levels

